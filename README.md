# ğŸ•¸ï¸ Turismo Scraper Uploader

Script en **Python + Flask** que realiza *web scraping* del portal oficial de turismo del PerÃº  
ğŸ‘‰ [Y TÃº QuÃ© Planes](https://www.ytuqueplanes.com/rutas-cortas),  
extrayendo informaciÃ³n sobre **rutas, regiones y lugares turÃ­sticos**,  
y subiendo automÃ¡ticamente los resultados a un bucket **Amazon S3** dentro de AWS.  

Este proceso representa la **Raw Layer (Storage Layer)** dentro de una arquitectura **Data Lakehouse**.

---

## ğŸš€ CaracterÃ­sticas

- ğŸŒ Extrae datos reales desde *ytuqueplanes.com/rutas-cortas* usando **Selenium**.  
- ğŸ§¾ Genera un archivo CSV con informaciÃ³n sobre rutas, coordenadas y descripciones.  
- â˜ï¸ Sube automÃ¡ticamente el CSV al bucket **S3** configurado.  
- ğŸ§  Ideal como fuente para **AWS Glue (Catalog Layer)** y **Athena (Consumption Layer)**.  
- âš™ï¸ Se puede invocar fÃ¡cilmente desde **Postman** mediante una llamada HTTP.

---

## âš™ï¸ Requisitos

- **Instancia EC2 (Ubuntu 22.04 o superior)**
- **Python 3.10+**
- **Rol IAM:** `LabRole` (con acceso a S3)
- **Google Chrome** y **ChromeDriver** instalados
- **LibrerÃ­as Python:** `Flask`, `Selenium`, `Boto3`, `Pandas`, `WebDriver-Manager`

---

## ğŸ§© Pasos de instalaciÃ³n en EC2

### 1ï¸âƒ£ Abrir la consola

- En la consola AWS, ve a **EC2 â†’ Instances â†’ selecciona tu instancia â†’ Connect**  
- Usa la pestaÃ±a **Session Manager** y haz clic en **Connect** para abrir la terminal web.

---

### 2ï¸âƒ£ Instalar dependencias bÃ¡sicas
```bash
sudo apt update -y
sudo apt install git python3-pip -y
```

---

### 3ï¸âƒ£ Clonar el repositorio
```bash
git clone https://github.com/utec-postgrado-ds-ia/turismo_scraper_uploader.git
cd turismo_scraper_uploader
```

Verifica que el script estÃ© presente:
```bash
ls
```
DeberÃ­as ver:
```
scraping_api.py
README.md
```

---

### 4ï¸âƒ£ Crear e instalar dependencias del scraping

Crea el archivo `requirements.txt` (si no existe):
```bash
echo "flask
pandas
boto3
selenium
webdriver-manager" > requirements.txt
```

Instala todo:
```bash
pip install -r requirements.txt
```

---

### 5ï¸âƒ£ Instalar Google Chrome y ChromeDriver

Instala Chrome:
```bash
sudo apt install unzip -y
wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
sudo apt install ./google-chrome-stable_current_amd64.deb -y
```

âš™ï¸ Ahora instala el ChromeDriver correcto (ajustado a tu versiÃ³n 142.0.7444.59):
```bash
CHROME_VERSION=142.0.7444.59
wget https://storage.googleapis.com/chrome-for-testing-public/${CHROME_VERSION}/linux64/chromedriver-linux64.zip
unzip chromedriver-linux64.zip
sudo mv chromedriver-linux64/chromedriver /usr/local/bin/chromedriver
sudo chmod +x /usr/local/bin/chromedriver
```

---

### 6ï¸âƒ£ Ejecutar el servidor Flask
```bash
python3 scraping_api.py
```

Si todo estÃ¡ correcto, deberÃ­as ver:
```
* Running on http://0.0.0.0:5000
```

---

## ğŸ§ª CÃ³mo probar con Postman

1. Abre el puerto **5000** en tu **Security Group**  
   *(Inbound Rule â†’ Custom TCP â†’ 5000 â†’ 0.0.0.0/0)*  
2. ObtÃ©n la **IP pÃºblica** de tu instancia (por ejemplo: `3.84.125.40`)  
3. En **Postman**, crea una nueva peticiÃ³n:

   - **MÃ©todo:** `POST`  
   - **URL:** `http://<tu-ip-ec2>:5000/generar-scraping`  
   - **Body:** vacÃ­o  

4. Presiona **Send**. Si todo sale bien, obtendrÃ¡s una respuesta como:
```json
{
    "status": "ok",
    "message": "Archivo subido correctamente a s3://turismo-datalake-31102025/raw/lugar_turistico/lugares_turisticos_20251031_123000.csv",
    "total_registros": 128
}
```

---

## ğŸª£ Bucket S3 esperado

**Bucket:** `turismo-datalake-31102025`

**Estructura recomendada:**
```
turismo-datalake-31102025/
â”‚
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ lugar_turistico/
â”‚       â””â”€â”€ lugares_turisticos_<fecha>.csv
â”‚
â”œâ”€â”€ silver/
â””â”€â”€ gold/
```

---

## ğŸ’¡ Notas finales

- Este scraper estÃ¡ pensado para uso acadÃ©mico y demostrativo dentro del curso de **AWS Data Lakehouse (UTEC)**.  
- Puede ampliarse fÃ¡cilmente para incluir mÃ¡s fuentes de datos (por ejemplo, **bases de datos o APIs**).  
- No requiere configurar credenciales IAM: el **LabRole** del entorno de UTEC ya tiene los permisos necesarios para S3.